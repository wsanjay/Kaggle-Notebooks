{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wsanjay/Kaggle-Notebooks/blob/main/%F0%9F%8C%8A_AutoBitnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dbsnrDKKVarI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cfa0ad32-4ffc-4924-8aa1-a14dbc59e4b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.6)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.45.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wandb_api_key' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7196ff9668bc>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_api_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wandb_api_key' is not defined"
          ]
        }
      ],
      "source": [
        "# @title # ðŸŒŠ AutoBitnet\n",
        "\n",
        "# @markdown Train your Bitnet model based on LLama Architecture for free on Colab T4 GPU.\n",
        "\n",
        "# @markdown ðŸ”® Created by [@zainulabideen](https://huggingface.co/abideen)\n",
        "\n",
        "# @markdown Based on [The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://arxiv.org/abs/2402.17764) paper\n",
        "\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### âœ¨ Model Parameters\n",
        "\n",
        "MODEL_CONFIG = \"NousResearch/Llama-2-7b-hf\" # @param {type:\"string\"}\n",
        "HEADS = 6 # @param {type: \"number\"}\n",
        "DIMENSIONS = 768 # @param {type: \"number\"}\n",
        "LAYERS = 6 # @param {type: \"number\"}\n",
        "INTERMEDIATE_SIZE= 1024 # @param {type: \"number\"}\n",
        "CONTEXT_LENGTH = 256 # @param {type: \"number\"}\n",
        "NEW_MODEL = \"Bitnet-Llama-70M\" # @param {type:\"string\"}\n",
        "HUGGINGFACE_ID = \"abideen\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### ðŸ’¥ Training Parameters\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get('hf-key') # @param {type:\"string\"}\n",
        "WANDB_TOKEN = userdata.get('wandb_api_key') # @param {type:\"string\"}\n",
        "DATASET = \"abideen/Cosmopedia-100k-pretrain\" # @param {type:\"string\"}\n",
        "BATCH_SIZE = 8 # @param {type:\"number\"}\n",
        "LEARNING_RATE = 1.5e-3 # @param {type:\"number\"}\n",
        "EPOCHS = 2 # @param {type:\"number\"}\n",
        "!pip install datasets wandb accelerate\n",
        "from torch import nn\n",
        "from transformers.models.llama.modeling_llama import *\n",
        "from transformers import (AutoTokenizer, AutoConfig, LlamaForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments)\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import login\n",
        "import wandb\n",
        "from huggingface_hub import create_repo, HfApi\n",
        "\n",
        "def activation_quant(x):\n",
        "    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n",
        "    y = (x * scale).round().clamp_(-128, 127) / scale\n",
        "    return y\n",
        "def weight_quant(w):\n",
        "    scale = 1.0 / w.abs().mean().clamp_(min=1e-5)\n",
        "    u = (w * scale).round().clamp_(-1, 1) / scale\n",
        "    return u\n",
        "\n",
        "class BitLinear(nn.Linear):\n",
        "    def forward(self, x):\n",
        "        w = self.weight # a weight tensor with shape [d, k]\n",
        "        x = x.to(w.device)\n",
        "        RMSNorm = LlamaRMSNorm(x.shape[-1]).to(w.device)\n",
        "        x_norm = RMSNorm(x)\n",
        "        # A trick for implementing Straightâˆ’Throughâˆ’Estimator (STE) using detach()\n",
        "        x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()\n",
        "        w_quant = w + (weight_quant(w) - w).detach()\n",
        "        y = F.linear(x_quant, w_quant)\n",
        "        return y\n",
        "\n",
        "def convert_to_bitnet(model, copy_weights):\n",
        "    for name, module in model.named_modules():\n",
        "        # Replace linear layers with BitNet\n",
        "        if isinstance(module, LlamaSdpaAttention) or isinstance(module, LlamaMLP):\n",
        "            for child_name, child_module in module.named_children():\n",
        "                if isinstance(child_module, nn.Linear):\n",
        "                    bitlinear = BitLinear(child_module.in_features, child_module.out_features, child_module.bias is not None).to(device=\"cuda:0\")\n",
        "                    if copy_weights:\n",
        "                        bitlinear.weight = child_module.weight\n",
        "                        if child_module.bias is not None:\n",
        "                            bitlinear.bias = child_module.bias\n",
        "                    setattr(module, child_name, bitlinear)\n",
        "        # Remove redundant input_layernorms\n",
        "        elif isinstance(module, LlamaDecoderLayer):\n",
        "            for child_name, child_module in module.named_children():\n",
        "                if isinstance(child_module, LlamaRMSNorm) and child_name == \"input_layernorm\":\n",
        "                    setattr(module, child_name, nn.Identity().to(device=\"cuda:0\"))\n",
        "\n",
        "\n",
        "wandb.login(key=wandb_api_key)\n",
        "login(token=hf-key)\n",
        "data = load_dataset(DATASET)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG)\n",
        "\n",
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=False,\n",
        "        max_length=CONTEXT_LENGTH,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    # Combine all tokens\n",
        "    combined = []\n",
        "    for tokenized_doc in outputs['input_ids']:\n",
        "        combined += tokenized_doc + [tokenizer.eos_token_id]\n",
        "    # Chunk\n",
        "    input_batch = []\n",
        "    for i in range(0, len(combined) - CONTEXT_LENGTH, CONTEXT_LENGTH):\n",
        "        input_batch.append(combined[i:i+CONTEXT_LENGTH])\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "tokenized_data = data.map(\n",
        "    tokenize, batched=True, remove_columns=data[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "total_tokens = tokenized_data['train'].num_rows * CONTEXT_LENGTH\n",
        "print(f\"Training on {total_tokens:_} tokens\")\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    MODEL_CONFIG,\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=CONTEXT_LENGTH,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "config.hidden_size = DIMENSIONS\n",
        "config.max_position_embeddings = DIMENSIONS\n",
        "config.num_attention_heads = HEADS\n",
        "config.num_hidden_layers = LAYERS\n",
        "config.num_key_value_heads = HEADS\n",
        "config.intermediate_size = INTERMEDIATE_SIZE\n",
        "\n",
        "### Create the llama model with our custom config. Convert it to bitnet.\n",
        "model = LlamaForCausalLM(config)\n",
        "convert_to_bitnet(model, copy_weights=False)\n",
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"Model size: {model_size/1000**2:.1f}M parameters\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "output_path = \"./out\"\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_path,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    logging_steps=100,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    save_steps=0.25,\n",
        "    fp16=True,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(f\"{output_path}/final_model\")\n",
        "folder = \"./out/final_model\"\n",
        "api = HfApi()\n",
        "# create_repo(\n",
        "#     repo_id = f\"{HUGGINGFACE_ID}/{NEW_MODEL}\",\n",
        "#     repo_type=\"model\",\n",
        "#     exist_ok=True,\n",
        "#     token=HF_TOKEN,\n",
        "# )\n",
        "\n",
        "# Upload gguf files\n",
        "# api.upload_folder(\n",
        "#     folder_path=folder,\n",
        "#     repo_type=\"model\",\n",
        "#     repo_id=f\"{HUGGINGFACE_ID}/{NEW_MODEL}\",\n",
        "#     token=HF_TOKEN,\n",
        "# )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30637,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fine-tune a Mistral-7b model with Direct Preference Optimization\nBoost the performance of your supervised fine-tuned models\n\nhttps://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac","metadata":{}},{"cell_type":"code","source":"%pip install -U datasets #TRL importation error on Kaggle #https://discuss.huggingface.co/t/trl-importation-error-on-kaggle/66817\n%pip install -U datasets trl peft bitsandbytes sentencepiece wandb ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install \"numpy>=1.16.5,<1.23.0\" ##https://stackoverflow.com/questions/73072257/resolve-warning-a-numpy-version-1-16-5-and-1-23-0-is-required-for-this-versi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install \"torch>=1.10\" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Restart notebook!!","metadata":{}},{"cell_type":"code","source":"from trl import DPOTrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport torch\n\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\nfrom trl import DPOTrainer\nimport bitsandbytes as bnb\n#from google.colab import userdata\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n# secret_value_0 = user_secrets.get_secret(\"hf-key\")\n# secret_value_1 = user_secrets.get_secret(\"wandb_api_key\")\n# Defined in the secrets tab in Google Colab\nhf_token = user_secrets.get_secret('hf-key')\nwb_token = user_secrets.get_secret('wandb_api_key')\nwandb.login(key=wb_token)\n\nmodel_name = \"teknium/OpenHermes-2.5-Mistral-7B\"\nnew_model = \"NeuralHermes-2.5-Mistral-7B\"\n\n\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:1024\"\ntorch.cuda.empty_cache() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chatml_format(example):\n    # Format system\n    if len(example['system']) > 0:\n        message = {\"role\": \"system\", \"content\": example['system']}\n        system = tokenizer.apply_chat_template([message], tokenize=False)\n    else:\n        system = \"\"\n\n    # Format instruction\n    message = {\"role\": \"user\", \"content\": example['question']}\n    prompt = tokenizer.apply_chat_template([message], tokenize=False, add_generation_prompt=True)\n\n    # Format chosen answer\n    chosen = example['chosen'] + \"<|im_end|>\\n\"\n\n    # Format rejected answer\n    rejected = example['rejected'] + \"<|im_end|>\\n\"\n\n    return {\n        \"prompt\": system + prompt,\n        \"chosen\": chosen,\n        \"rejected\": rejected,\n    }\n\n# Load dataset\ndataset = load_dataset(\"Intel/orca_dpo_pairs\")['train']\n\n# Save columns\noriginal_columns = dataset.column_names\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"left\"\n\n# Format dataset\ndataset = dataset.map(\n    chatml_format,\n    remove_columns=original_columns\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LoRA configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nlogger = logging.getLogger(__name__)\ndevice_count = torch.cuda.device_count()\nif device_count > 0:\n    logger.debug(\"Select GPU device\")\n    device = torch.device(\"cuda\")\nelse:\n    logger.debug(\"Select CPU device\")\n    device = torch.device(\"cpu\")\n    \n\n\ndevice","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model to fine-tune\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n    low_cpu_mem_usage=True, \n)\nmodel.config.use_cache = False\n\n# Reference model\nref_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,\n    load_in_4bit=True,\n    low_cpu_mem_usage=True, \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    gradient_checkpointing=True,\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    max_steps=200,\n    save_strategy=\"no\",\n    logging_steps=1,\n    output_dir=new_model,\n    optim=\"paged_adamw_32bit\",\n    warmup_steps=100,\n    #bf16=True,  #https://github.com/tatsu-lab/stanford_alpaca/issues/133\n    fp16=True,\n    report_to=\"wandb\",\n)\n\n# Create DPO trainer\ndpo_trainer = DPOTrainer(\n    model,\n    ref_model,\n    args=training_args,\n    train_dataset=dataset,\n    tokenizer=tokenizer,\n    peft_config=peft_config,\n    beta=0.1,\n    max_prompt_length=1024,\n    max_length=1536,\n)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fine-tune model with DPO\ndpo_trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6829369,"sourceType":"datasetVersion","datasetId":3926868}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-02T13:12:11.221406Z","iopub.execute_input":"2023-11-02T13:12:11.221810Z","iopub.status.idle":"2023-11-02T13:12:11.669989Z","shell.execute_reply.started":"2023-11-02T13:12:11.221775Z","shell.execute_reply":"2023-11-02T13:12:11.668734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install transformers==4.26.0 datasets botocore==1.31.17\n!pip install huggingface \n!pip install jsonlines config #lamini \n!pip install 'transformers[torch]'==4.26.0\n!pip install git+https://github.com/huggingface/accelerate","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:12:30.336377Z","iopub.execute_input":"2023-11-22T01:12:30.336946Z","iopub.status.idle":"2023-11-22T01:13:52.876653Z","shell.execute_reply.started":"2023-11-22T01:12:30.336918Z","shell.execute_reply":"2023-11-22T01:13:52.875633Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama import BasicModelRunner\n\nmodel = BasicModelRunner(\"EleutherAI/pythia-410m\") \nmodel.load_data_from_jsonlines(\"/kaggle/input/data-set/seed_lamini_docs.jsonl\", input_key=\"question\", output_key=\"answer\")\n# model.train(is_public=True) ","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datasets\nimport tempfile\nimport logging\nimport random\nimport config\nimport os\nimport yaml\nimport time\nimport torch\nimport transformers\nimport pandas as pd\nimport jsonlines\n\n#from utilities import *\nfrom transformers import AutoTokenizer\n#from transformers import AutoModelForCausalLM\nfrom transformers import TrainingArguments\nfrom transformers import AutoModelForCausalLM\n#from llama import BasicModelRunner\n\n\nlogger = logging.getLogger(__name__)\nglobal_config = None","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:14:21.570772Z","iopub.execute_input":"2023-11-22T01:14:21.571149Z","iopub.status.idle":"2023-11-22T01:14:34.257802Z","shell.execute_reply.started":"2023-11-22T01:14:21.571117Z","shell.execute_reply":"2023-11-22T01:14:34.256793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_name = \"lamini_docs.jsonl\"\ndataset_path = f\"/content/{dataset_name}\"\nuse_hf = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = \"lamini/lamini_docs\"\nuse_hf = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set up the model, training config, and tokenize","metadata":{}},{"cell_type":"code","source":"device_count = torch.cuda.device_count()\nif device_count > 0:\n    logger.debug(\"Select GPU device\")\n    device = torch.device(\"cuda\")\nelse:\n    logger.debug(\"Select CPU device\")\n    device = torch.device(\"cpu\")\n    \n\n\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:14:34.259335Z","iopub.execute_input":"2023-11-22T01:14:34.259920Z","iopub.status.idle":"2023-11-22T01:14:34.277456Z","shell.execute_reply.started":"2023-11-22T01:14:34.259891Z","shell.execute_reply":"2023-11-22T01:14:34.276435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"facebook/opt-350m\" #\"EleutherAI/pythia-70m-deduped\"#\"EleutherAI/pythia-70m-deduped\"\nfrom transformers import GPTNeoXForCausalLM, AutoTokenizer\n\nbase_model = AutoModelForCausalLM.from_pretrained(   #GPTNeoXForCausalLM\n  model_name,\n  #revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n).to(device)\n\ntokenizer = AutoTokenizer.from_pretrained(\n  model_name,\n  #revision=\"step3000\",\n  cache_dir=\"./pythia-70m-deduped/step3000\",\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:22:26.549327Z","iopub.execute_input":"2023-11-22T01:22:26.550074Z","iopub.status.idle":"2023-11-22T01:22:31.214429Z","shell.execute_reply.started":"2023-11-22T01:22:26.550036Z","shell.execute_reply":"2023-11-22T01:22:31.213403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    if \"question\" in examples and \"answer\" in examples:\n      if len(examples[\"question\"][0].strip()) == 0:   #examples[\"question\"][0] =='' or \n        examples[\"question\"][0] ='#'\n      if len(examples[\"answer\"][0].strip()) == 0:\n        examples[\"answer\"][0] ='#'\n      text = examples[\"question\"][0] + examples[\"answer\"][0]\n    elif \"input\" in examples and \"output\" in examples:\n      text = examples[\"input\"][0] + examples[\"output\"][0]\n    else:\n      text = examples[\"text\"][0]\n\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"np\",\n        padding=True,\n    )\n\n    max_length = min(\n        tokenized_inputs[\"input_ids\"].shape[1],\n        2048\n    )\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"np\",\n        truncation=True,\n        max_length=max_length,\n        padding=True\n    )\n    tokenized_inputs['labels'] = tokenized_inputs['input_ids'][:]\n    return tokenized_inputs","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:23:36.376232Z","iopub.execute_input":"2023-11-22T01:23:36.377082Z","iopub.status.idle":"2023-11-22T01:23:36.385306Z","shell.execute_reply.started":"2023-11-22T01:23:36.377050Z","shell.execute_reply":"2023-11-22T01:23:36.384295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filename= \"/kaggle/input/data-set/seed_lamini_docs.jsonl\"\nfinetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n# finetuning_dataset_loaded = datasets.load_dataset(\"kotzeje/tokenized_lamini_docs\")\ntokenized_dataset = finetuning_dataset_loaded.map(\n    tokenize_function,\n    batched=True,\n    batch_size=1,\n    drop_last_batch=True\n)\n\nprint(tokenized_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:23:40.906462Z","iopub.execute_input":"2023-11-22T01:23:40.906848Z","iopub.status.idle":"2023-11-22T01:23:45.677910Z","shell.execute_reply.started":"2023-11-22T01:23:40.906820Z","shell.execute_reply":"2023-11-22T01:23:45.677016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenized_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:23:48.643213Z","iopub.execute_input":"2023-11-22T01:23:48.644068Z","iopub.status.idle":"2023-11-22T01:23:48.649057Z","shell.execute_reply.started":"2023-11-22T01:23:48.644033Z","shell.execute_reply":"2023-11-22T01:23:48.648129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.DataFrame(tokenized_dataset)\n\ndf_filtered = df[df['question'] == '#'].index\n# questdf_filteredon=df_filtered.fillna(\"NaN\")\n# df[modifiedDf['column_name'] == ''].index\n\ndf.drop(df_filtered , inplace=True)\nprint(df.columns)\n#df.drop(columns=['__index_level_0__'],inplace=True)\nfrom datasets import Dataset\ntokenized_dataset= Dataset.from_pandas(df)\ntokenized_dataset = tokenized_dataset.remove_columns('__index_level_0__')\ntokenized_dataset","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:23:53.002078Z","iopub.execute_input":"2023-11-22T01:23:53.002467Z","iopub.status.idle":"2023-11-22T01:23:53.590857Z","shell.execute_reply.started":"2023-11-22T01:23:53.002434Z","shell.execute_reply":"2023-11-22T01:23:53.589754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#This was the bogus step \n#tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_dataset = tokenized_dataset.train_test_split(test_size=0.12, shuffle=True, seed=11)\n\n#print(split_dataset)\ntrain_dataset, test_dataset =split_dataset['train'] , split_dataset['test'] \nprint(train_dataset)\nprint(test_dataset)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:24:07.761080Z","iopub.execute_input":"2023-11-22T01:24:07.761481Z","iopub.status.idle":"2023-11-22T01:24:07.775872Z","shell.execute_reply.started":"2023-11-22T01:24:07.761450Z","shell.execute_reply":"2023-11-22T01:24:07.774842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print( train_dataset['input_ids'][960][0], train_dataset['labels'][960][0] )\n#print( len(train_dataset['input_ids'][22][0]), len(train_dataset['labels'][22][0]) )","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:24:13.370477Z","iopub.execute_input":"2023-11-22T01:24:13.370873Z","iopub.status.idle":"2023-11-22T01:24:13.534234Z","shell.execute_reply.started":"2023-11-22T01:24:13.370842Z","shell.execute_reply":"2023-11-22T01:24:13.533157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load the base model**","metadata":{}},{"cell_type":"code","source":"#base_model = AutoModelForCausalLM.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define function to carry out inference**","metadata":{}},{"cell_type":"markdown","source":"**Setup training**","metadata":{}},{"cell_type":"code","source":"!pip install evaluate nltk rouge_score","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n# Setup evaluation\nnltk.download(\"punkt\", quiet=True)\nmetric = evaluate.load(\"rouge\")\n\ndef compute_metrics(eval_preds):\n    preds, labels = eval_preds\n\n    # decode preds and labels\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # rougeLSum expects newline after each sentence\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    return result\ndef compute_rouge_score(generated, reference):\n    \"\"\"\n    Compute ROUGE scores on a batch of articles.\n\n    This is a convenience function wrapping Hugging Face `rouge_score`,\n    which expects sentences to be separated by newlines.\n\n    :param generated: Summaries (list of strings) produced by the model\n    :param reference: Ground-truth summaries (list of strings) for comparison\n    \"\"\"\n    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n    return metric.compute(\n        predictions=generated_with_newlines,\n        references=reference_with_newlines,\n        use_stemmer=True,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-11-04T16:00:11.868332Z","iopub.execute_input":"2023-11-04T16:00:11.869030Z","iopub.status.idle":"2023-11-04T16:00:12.509243Z","shell.execute_reply.started":"2023-11-04T16:00:11.868999Z","shell.execute_reply":"2023-11-04T16:00:12.507955Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_steps ='3-epochs'\ntrained_model_name = f\"lamini_docs_{max_steps}_steps\"\noutput_dir = trained_model_name","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:24:42.332027Z","iopub.execute_input":"2023-11-22T01:24:42.333235Z","iopub.status.idle":"2023-11-22T01:24:42.337611Z","shell.execute_reply.started":"2023-11-22T01:24:42.333196Z","shell.execute_reply":"2023-11-22T01:24:42.336537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n\n  # Learning rate\n  learning_rate=1.5e-5,\n\n  # Number of training epochs\n  num_train_epochs=3,\n\n  # Max steps to train for (each step is a batch of data)\n  # Overrides num_train_epochs, if not -1\n  #max_steps=max_steps,\n\n  # Batch size for training\n  per_device_train_batch_size=1,\n\n  # Directory to save model checkpoints\n  output_dir=output_dir,\n\n  # Other arguments\n  overwrite_output_dir=False, # Overwrite the content of the output directory\n  disable_tqdm=False, # Disable progress bars\n  eval_steps=120, # Number of update steps between two evaluations\n  save_steps=120, # After # steps model is saved\n  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n  per_device_eval_batch_size=1, # Batch size for evaluation\n  evaluation_strategy=\"steps\",\n  logging_strategy=\"steps\",\n  logging_steps=1,\n  optim=\"adafactor\",\n  #optim=\"adamw_torch\",\n  gradient_accumulation_steps = 4,\n  gradient_checkpointing=False,\n\n  # Parameters for early stopping\n  load_best_model_at_end=True,\n  save_total_limit=3,\n  metric_for_best_model= \"eval_loss\",\n  greater_is_better=False,\n\n#\n  weight_decay=0.01,\n  \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:24:45.980908Z","iopub.execute_input":"2023-11-22T01:24:45.981792Z","iopub.status.idle":"2023-11-22T01:24:45.994460Z","shell.execute_reply.started":"2023-11-22T01:24:45.981757Z","shell.execute_reply":"2023-11-22T01:24:45.993580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_hf = False\ntraining_config = {\n    \"model\": {\n        \"pretrained_name\": model_name,\n        \"max_length\" : 2048\n    },\n    \"datasets\": {\n        \"use_hf\": use_hf, #use_hf = False\n        \"path\": filename    #filename= \"/kaggle/input/data-set/seed_lamini_docs.jsonl\"\n    },\n    \"verbose\": True\n}\n\nmodel_flops = (\n  base_model.floating_point_ops(\n    {\n       \"input_ids\": torch.zeros(\n           (1, training_config[\"model\"][\"max_length\"])\n      )\n    }\n  )\n  * training_args.gradient_accumulation_steps\n)\n\nprint(base_model)\nprint(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\nprint(\"Flops\", model_flops / 1e9, \"GFLOPs\")","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nmetric = load_metric(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\nfrom transformers import Trainer\ntrainer = Trainer(\n    model=base_model,\n#     model_flops=model_flops,\n#     total_steps=max_steps,\n    args= training_args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n#     compute_metrics=compute_rouge_score, \n)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:24:52.055059Z","iopub.execute_input":"2023-11-22T01:24:52.055920Z","iopub.status.idle":"2023-11-22T01:24:53.039198Z","shell.execute_reply.started":"2023-11-22T01:24:52.055883Z","shell.execute_reply":"2023-11-22T01:24:53.038241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:24:55.643835Z","iopub.execute_input":"2023-11-22T01:24:55.644665Z","iopub.status.idle":"2023-11-22T01:32:24.867987Z","shell.execute_reply.started":"2023-11-22T01:24:55.644629Z","shell.execute_reply":"2023-11-22T01:32:24.867037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = '/kaggle/input/'\nsave_dir = '/final'\n\ntrainer.save_model(save_dir)\nprint(\"Saved model to:\", save_dir)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:32:47.441452Z","iopub.execute_input":"2023-11-22T01:32:47.441860Z","iopub.status.idle":"2023-11-22T01:32:49.325140Z","shell.execute_reply.started":"2023-11-22T01:32:47.441824Z","shell.execute_reply":"2023-11-22T01:32:49.323911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:32:53.498032Z","iopub.execute_input":"2023-11-22T01:32:53.498866Z","iopub.status.idle":"2023-11-22T01:32:56.769567Z","shell.execute_reply.started":"2023-11-22T01:32:53.498833Z","shell.execute_reply":"2023-11-22T01:32:56.768550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"finetuned_slightly_model.to(device) ","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:33:00.484422Z","iopub.execute_input":"2023-11-22T01:33:00.484880Z","iopub.status.idle":"2023-11-22T01:33:00.829718Z","shell.execute_reply.started":"2023-11-22T01:33:00.484836Z","shell.execute_reply":"2023-11-22T01:33:00.828817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def inference(text, model, tokenizer, max_input_tokens=1024, max_output_tokens=100):\n  # Tokenize\n  input_ids = tokenizer.encode(\n          text,\n          return_tensors=\"pt\",\n          truncation=True,\n          max_length=max_input_tokens\n  )\n\n  # Generate\n  device = model.device\n#   generated_tokens_with_prompt = model.generate(\n#     input_ids=input_ids.to(device),\n#     #max_length=max_output_tokens,\n#     max_new_tokens=max_output_tokens,\n#     pad_token_id=tokenizer.eos_token_id,\n#     eos_token_id=model.config.eos_token_id,\n#     output_scores=True,\n#     num_beams=5,\n#     #num_return_sequences=3,\n    \n#     # num_beam_groups=5,\n#     # diversity_penalty=1.0,\n    \n#     no_repeat_ngram_size=3,\n#     early_stopping=True,\n#     #temperature=0.6,\n#     do_sample=True,\n\n#   )\n\n  generated_tokens_with_prompt = model.generate(\n    input_ids=input_ids.to(device),\n    max_length=max_output_tokens,\n    #max_new_tokens=max_output_tokens,\n    pad_token_id=tokenizer.eos_token_id,\n    eos_token_id=model.config.eos_token_id,\n    output_scores=True,\n    penalty_alpha=0.4, top_k=4,\n    num_beams=5,\n    #num_return_sequences=2,\n    \n    # num_beam_groups=5,\n    # diversity_penalty=1.0,\n    \n    no_repeat_ngram_size=3,\n    early_stopping=True,\n    #temperature=0.6,\n    do_sample=True,\n  )\n\n  # Decode\n  generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n\n  # Strip the prompt\n  generated_text_answer = generated_text_with_prompt[0][len(text):]\n\n  return generated_text_answer","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:33:09.207129Z","iopub.execute_input":"2023-11-22T01:33:09.207516Z","iopub.status.idle":"2023-11-22T01:33:09.217741Z","shell.execute_reply.started":"2023-11-22T01:33:09.207469Z","shell.execute_reply":"2023-11-22T01:33:09.216733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\ntest_question = test_dataset[55]['question']\nprint(\"Question input (test):\", test_question)\n\nprint(\"Finetuned slightly model's answer: \")\ngenerated = inference(test_question, finetuned_slightly_model, tokenizer)\npprint(generated)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:34:01.899241Z","iopub.execute_input":"2023-11-22T01:34:01.900098Z","iopub.status.idle":"2023-11-22T01:34:03.565264Z","shell.execute_reply.started":"2023-11-22T01:34:01.900064Z","shell.execute_reply":"2023-11-22T01:34:03.564193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_answer = test_dataset[25]['answer']\nprint(\"Target answer output (test):\", test_answer)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:33:41.066742Z","iopub.execute_input":"2023-11-22T01:33:41.067099Z","iopub.status.idle":"2023-11-22T01:33:41.075851Z","shell.execute_reply.started":"2023-11-22T01:33:41.067071Z","shell.execute_reply":"2023-11-22T01:33:41.074852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# TODO\nrouge_scores = compute_rouge_score(generated, test_answer)","metadata":{"execution":{"iopub.status.busy":"2023-11-22T01:33:46.685599Z","iopub.execute_input":"2023-11-22T01:33:46.685983Z","iopub.status.idle":"2023-11-22T01:33:46.834509Z","shell.execute_reply.started":"2023-11-22T01:33:46.685955Z","shell.execute_reply":"2023-11-22T01:33:46.833270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install autotrain-advanced","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}